# LLaMa Utils (WIP)

## LLaMa2 patching for flash-attention 1 or 2

Why you may ask I support flash-attention 1 and 2? Because I run this model in Colab and I don't wanna execute everything in A100 (Expensive GPU) and T4 can only run Flash-attention 1. So I have to support both.